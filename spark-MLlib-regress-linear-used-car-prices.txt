---- Feature extraction & Data Munging --------------

val rdd = sc.textFile("used_cars_price/used_cars_price*.csv").map(x => x.split(","))

// checking array size is not uniform
rdd.map(x => (x.size,1)).reduceByKey(_+_).take(10)
res45: Array[(Int, Int)] = Array((14,6019), (12,1041), (13,182), (11,1), (9,10))

// considering only samples with 13 or 14 attributes
val rdd1 = rdd.filter(x => x.size > 12)

// for numerical columns, removing embebbed chars and converting to doubles
val rdd2 = rdd1.map( x => {
   val age = if (x(3).isEmpty) 0 else 2020 - x(3).toDouble
   val km_driven = if (x(4).isEmpty) 0 else x(4).toDouble/1000
   //
   val mileage_aux = ("""[0-9]+\.[0-9]+""".r findAllIn x(8)).toList
   val mileage = if (mileage_aux.isEmpty) 0 else mileage_aux.head.toDouble
   //
   val engine_aux = ("""[0-9]+\.[0-9]+""".r findAllIn x(9)).toList
   val engine = if (engine_aux.isEmpty) 0 else engine_aux.head.toDouble
   //
   val power_aux = ("""[0-9]+\.[0-9]+""".r findAllIn x(10)).toList
   val power = if (power_aux.isEmpty) 0 else power_aux.head.toDouble
   //
   val seats = if (x(11).isEmpty) 0 else x(11).toDouble
   val price = if (x.size == 13) -1 else x(13).toDouble
   //
   Array(age,km_driven,mileage,engine,power,seats,price)
 })
 
val rdd3 = rdd1.map( x => Array(x(1),x(2),x(5),x(6),x(7)))

---- Conversion to 1-to-k binary encoding vectors 

def oneHotEncColumns(rddx: org.apache.spark.rdd.RDD[Array[String]], idx: Int):org.apache.spark.rdd.RDD[Array[Double]] = {
  val categories = rddx.map(r => r(idx)).distinct.zipWithIndex.collect.toMap
  val numCategories = categories.size
  val vetcateg = rddx.map(r => {
      val categoryIdx = categories(r(idx)).toInt
      val categoryFeatures = Array.ofDim[Double](numCategories)
      categoryFeatures(categoryIdx) = 1.0
      categoryFeatures
  })
  vetcateg
}

def mergeArray(rddx: org.apache.spark.rdd.RDD[Array[String]], idx: Int*):org.apache.spark.rdd.RDD[Array[Double]] = {
  var i = 0
  var arr1 = oneHotEncColumns(rddx,idx(i))
  for (j <- 1 until idx.size) {
    var arr2 = oneHotEncColumns(rddx,idx(j))
    var flt1 = arr1.zip(arr2).map(x => (x._1.toList ++ x._2.toList).toArray)
    arr1 = flt1
  }
  arr1
}

val concat = mergeArray(rdd3,0,1,2,3,4)

concat.first.size
res3: Int = 1943

// merging the numerical columns with 1-of-k vectors produced
val vect = concat.zip(rdd2).map(x => (x._1.toList ++ x._2.toList).toArray)

vect.first.size
res4: Int = 1950

import org.apache.spark.mllib.linalg.Vectors
import org.apache.spark.mllib.regression.LabeledPoint

val data = vect.map(x => {
  val arr_size = x.size - 1 
  val l = x(arr_size)
  val f = x.slice(0,arr_size)
  LabeledPoint(l,Vectors.dense(f))
})

val unlabeledSet = data.filter{ case LabeledPoint(l,f) => l == -1 }

val labeledSet = data.filter{ case LabeledPoint(l,f) => l != -1 }

val sets = labeledSet.randomSplit(Array(0.8,0.2))
val trainSet = sets(0)
val testSet = sets(1)

trainSet.cache

---- MLlib Linear regression --------------

import org.apache.spark.mllib.regression.LinearRegressionWithSGD
val alg = new LinearRegressionWithSGD()
alg.setIntercept(true)
alg.optimizer.setNumIterations(200)

---- step size = 1.0
alg.optimizer.setStepSize(1.0)
val model = alg.run(trainSet)
model: intercept = NaN, numFeatures = 1949

---- step size = 0.1
alg.optimizer.setStepSize(0.1)
val model = alg.run(trainSet)
model: intercept = NaN, numFeatures = 1949

---- step size = 0.01
alg.optimizer.setStepSize(0.01)
val model = alg.run(trainSet)
intercept = -9.797466240827234E241, numFeatures = 1949

val validPredicts = testSet.map(x => (model.predict(x.features),x.label))

validPredicts.take(10)
res14: Array[(Double, Double)] = Array((-1.991213363096325E246,17.5), (-3.232405203090325E246,15.0), (-4.61129978631197E245,5.99), (-1.9766061965049363E246,6.34), (-5.830781483721223E245,18.55), (-2.3635347486851983E246,23.5), (-1.4619652880161276E246,2.75), (-1.1995121819958606E246,12.5), (-4.0802005397370816E245,5.58), (-1.2829322511326366E246,4.74))

---- step size = 0.001
alg.optimizer.setStepSize(0.001)
val model = alg.run(trainSet)
intercept = -170.47897827907303, numFeatures = 1949

val validPredicts = testSet.map(x => (model.predict(x.features),x.label))

validPredicts.take(10)
res16: Array[(Double, Double)] = Array((-464.5120761855946,17.5), (-829.291416193739,15.0), (5363.930644793493,5.99), (964.7119980750163,6.34), (1011.71276451739,18.55), (-197.68768293077989,23.5), (-161.01554839641773,2.75), (-3300.1607820856657,12.5), (4275.257226879819,5.58), (-93.0653214906286,4.74))

---- step size = 0.0001
alg.optimizer.setStepSize(0.0001)
val model = alg.run(trainSet)
intercept = 1.0015816145186838, numFeatures = 1949

val validPredicts = testSet.map(x => (model.predict(x.features),x.label))

validPredicts.take(10)
res18: Array[(Double, Double)] = Array((14.878754895583658,17.5), (16.405824634282347,15.0), (2.0831743302379353,5.99), (9.091274633070366,6.34), (2.104373493514226,18.55), (15.638210795633082,23.5), (7.470855132624997,2.75), (2.491276719168561,12.5), (1.9775209378330723,5.58), (8.314215673622233,4.74))

import org.apache.spark.mllib.evaluation.RegressionMetrics
val validMetrics = new RegressionMetrics(validPredicts)
validMetrics.rootMeanSquaredError  //  11.504595467885252
validMetrics.meanSquaredError  // 132.3557168796859

---- step size = 0.0005
alg.optimizer.setStepSize(0.0005)
val model = alg.run(trainSet)
intercept = 1.0102525792960326, numFeatures = 1949

val validPredicts = testSet.map(x => (model.predict(x.features),x.label))

validPredicts.take(10)
res26: Array[(Double, Double)] = Array((14.15399873748867,17.5), (15.385211684726384,15.0), (4.464576858371174,5.99), (9.700422898653235,6.34), (4.065797194351171,18.55), (14.774093572146812,23.5), (8.407634321192258,2.75), (3.4601688930254513,12.5), (4.185663075333736,5.58), (8.976387021018228,4.74))

import org.apache.spark.mllib.evaluation.RegressionMetrics
val validMetrics = new RegressionMetrics(validPredicts)
validMetrics.rootMeanSquaredError  //  11.364581798037191
validMetrics.meanSquaredError  // 129.15371944427824

---- step size = 0.00001
alg.optimizer.setStepSize(0.00001)
val model = alg.run(trainSet)
intercept = 1.0004357267718385, numFeatures = 1949

val validPredicts = testSet.map(x => (model.predict(x.features),x.label))

validPredicts.take(10)
res22: Array[(Double, Double)] = Array((7.727216095212051,17.5), (9.29427485776231,15.0), (1.6918418025464184,5.99), (5.5428228023450385,6.34), (1.8112642237307863,18.55), (8.308627435811998,23.5), (4.522641936361604,2.75), (2.48165950423183,12.5), (1.6164198627019295,5.58), (4.698053507657101,4.74))

import org.apache.spark.mllib.evaluation.RegressionMetrics
val validMetrics = new RegressionMetrics(validPredicts)
validMetrics.rootMeanSquaredError  //  12.237336007906217
validMetrics.meanSquaredError  // 149.7523925703981

---- step size = 0.00005
alg.optimizer.setStepSize(0.00005)
val model = alg.run(trainSet)
intercept = 1.0011455805184137, numFeatures = 1949

val validPredicts = testSet.map(x => (model.predict(x.features),x.label))

validPredicts.take(10)
res30: Array[(Double, Double)] = Array((13.949586721770318,17.5), (15.689564692049599,15.0), (2.0021513845970023,5.99), (8.751256576679351,6.34), (2.075120574259633,18.55), (14.739727949232963,23.5), (7.144065752357653,2.75), (2.646149233479308,12.5), (1.9001458621527823,5.58), (7.850387060538628,4.74))

import org.apache.spark.mllib.evaluation.RegressionMetrics
val validMetrics = new RegressionMetrics(validPredicts)
validMetrics.rootMeanSquaredError  //  11.52304257465015
validMetrics.meanSquaredError  // 132.78051017719997


----- Decide to scale features because variabiliaty increases even reducing step size of model 

import org.apache.spark.mllib.feature.StandardScaler
val scaler = new StandardScaler(true, true).fit(trainSet.map(x => x.features))
 
val trainScaled = trainSet.map(x => LabeledPoint(x.label,scaler.transform(x.features)))
val validScaled = testSet.map(x => LabeledPoint(x.label, scaler.transform(x.features)))

---- MLlib Linear regression --------------

import org.apache.spark.mllib.regression.LinearRegressionWithSGD
val alg = new LinearRegressionWithSGD()
alg.setIntercept(true)
alg.optimizer.setNumIterations(200)

---- step size = 1.0
alg.optimizer.setStepSize(1.0)
val model = alg.run(trainScaled)
intercept = 9.43651264896509, numFeatures = 1949

val validPredicts = validScaled.map(x => (model.predict(x.features),x.label))

validPredicts.take(10)
res37: Array[(Double, Double)] = Array((15.60902589768709,17.5), (11.838342155148379,15.0), (7.716381067500517,5.99), (8.570731398668219,6.34), (12.801002308821754,18.55), (23.279912929891946,23.5), (2.6380436016036146,2.75), (12.108075552892132,12.5), (10.101245205089235,5.58), (3.802877782465237,4.74)

import org.apache.spark.mllib.evaluation.RegressionMetrics
val validMetrics = new RegressionMetrics(validPredicts)
validMetrics.rootMeanSquaredError  //  5.642255859016723
validMetrics.meanSquaredError  // 31.835051178608538

---- step size = 0.1
alg.optimizer.setStepSize(0.1)
val model = alg.run(trainScaled)
intercept = 8.470161565458957, numFeatures = 1949

val validPredicts = validScaled.map(x => (model.predict(x.features),x.label))

validPredicts.take(10)
res41: Array[(Double, Double)] = Array((16.154630954505535,17.5), (11.387306180943117,15.0), (5.302945119005102,5.99), (8.237605934975601,6.34), (12.563004826905956,18.55), (21.4997418068495,23.5), (1.1334270748233246,2.75), (12.05578971671771,12.5), (8.890466479976991,5.58), (3.1242265548293187,4.74))

import org.apache.spark.mllib.evaluation.RegressionMetrics
val validMetrics = new RegressionMetrics(validPredicts)
validMetrics.rootMeanSquaredError  //  5.8649788844225315
validMetrics.meanSquaredError  // 34.39797731472216

---- step size = 0.01
alg.optimizer.setStepSize(0.01)
val model = alg.run(trainScaled)
intercept = 2.9890818409191087, numFeatures = 1949

val validPredicts = validScaled.map(x => (model.predict(x.features),x.label))

validPredicts.take(10)
res45: Array[(Double, Double)] = Array((10.801529033709162,17.5), (4.973892514220564,15.0), (0.3856341359514488,5.99), (3.1044072638143088,6.34), (7.480758790687561,18.55), (9.893135026838284,23.5), (-1.9466285347996828,2.75), (7.995363996032808,12.5), (3.2559644844585214,5.58), (0.11245814799512965,4.74))

import org.apache.spark.mllib.evaluation.RegressionMetrics
val validMetrics = new RegressionMetrics(validPredicts)
validMetrics.rootMeanSquaredError  //  10.338478984508244
validMetrics.meanSquaredError  // 106.88414771311861

--------------------

import org.apache.spark.mllib.linalg.Matrix
import org.apache.spark.mllib.linalg.distributed.RowMatrix

val vectors = rdd2.map( x => Vectors.dense(x))
val matrix = new RowMatrix(vectors)

val colsims = matrix.columnSimilarities()
val mat1 = colsims.toRowMatrix

import org.apache.spark.mllib.linalg.distributed.MatrixEntry
val transformedRDD = colsims.entries.map{case MatrixEntry(row: Long, col:Long, sim:Double) => ((row,col),sim)}

val rep = transformedRDD.sortBy(_._1).map(x => ((x._1._1,x._1._2),x._2))

var i = -1.0

rep.foreach( x => {
  val sim = x._2
  if (x._1._1 != i) { println
    print(f"$sim%.4f ")
    i = x._1._1
  } else print(f"$sim%.4f ")
})

0.5520 0.8327 0.5830 0.8772 0.4732
0.5121 0.3569 0.5443
0.3408
0.6872 0.9462 0.5573
0.7123 0.5742
0.6328
