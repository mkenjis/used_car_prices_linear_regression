---- Feature extraction & Data Munging --------------

val rdd = sc.textFile("used_cars_price/used_cars_price*.csv").map(x => x.split(","))

// checking array size is not uniform
rdd.map(x => (x.size,1)).reduceByKey(_+_).take(10)
res45: Array[(Int, Int)] = Array((14,6019), (12,1041), (13,182), (11,1), (9,10))

// considering only samples with 13 or 14 attributes
val rdd1 = rdd.filter(x => x.size > 12)

// for numerical columns, removing embebbed chars and converting to doubles
val rdd2 = rdd1.map( x => {
   val age = if (x(3).isEmpty) 0 else 2020 - x(3).toDouble
   val km_driven = if (x(4).isEmpty) 0 else x(4).toDouble/1000
   //
   val mileage_aux = ("""[0-9]+\.[0-9]+""".r findAllIn x(8)).toList
   val mileage = if (mileage_aux.isEmpty) 0 else mileage_aux.head.toDouble
   //
   val engine_aux = ("""[0-9]+\.[0-9]+""".r findAllIn x(9)).toList
   val engine = if (engine_aux.isEmpty) 0 else engine_aux.head.toDouble
   //
   val power_aux = ("""[0-9]+\.[0-9]+""".r findAllIn x(10)).toList
   val power = if (power_aux.isEmpty) 0 else power_aux.head.toDouble
   //
   val seats = if (x(11).isEmpty) 0 else x(11).toDouble
   val price = if (x.size == 13) -1 else x(13).toDouble
   //
   Array(age,km_driven,mileage,engine,power,seats,price)
 })

// model description is only maker + model
val rdd3 = rdd1.map( x => {
  val model_car = x(1).substring(0,x(1).indexOf(' ',x(1).indexOf(' ',1)+1))
  Array(model_car,x(2),x(5),x(6),x(7))
})

---- Conversion to 1-to-k binary encoding vectors 

def oneHotEncColumns(rddx: org.apache.spark.rdd.RDD[Array[String]], idx: Int):org.apache.spark.rdd.RDD[Array[Double]] = {
  val categories = rddx.map(r => r(idx)).distinct.zipWithIndex.collect.toMap
  val numCategories = categories.size
  val vetcateg = rddx.map(r => {
      val categoryIdx = categories(r(idx)).toInt
      val categoryFeatures = Array.ofDim[Double](numCategories)
      categoryFeatures(categoryIdx) = 1.0
      categoryFeatures
  })
  vetcateg
}

def mergeArray(rddx: org.apache.spark.rdd.RDD[Array[String]], idx: Int*):org.apache.spark.rdd.RDD[Array[Double]] = {
  var i = 0
  var arr1 = oneHotEncColumns(rddx,idx(i))
  for (j <- 1 until idx.size) {
    var arr2 = oneHotEncColumns(rddx,idx(j))
    var flt1 = arr1.zip(arr2).map(x => (x._1.toList ++ x._2.toList).toArray)
    arr1 = flt1
  }
  arr1
}

val concat = mergeArray(rdd3,0,1,2,3,4)

concat.first.size
res58: Int = 239

// merging the numerical columns with 1-of-k vectors produced
val vect = concat.zip(rdd2).map(x => (x._1.toList ++ x._2.toList).toArray)

vect.first.size
res59: Int = 246

import org.apache.spark.mllib.linalg.Vectors
import org.apache.spark.mllib.regression.LabeledPoint

val data = vect.map(x => {
  val arr_size = x.size - 1 
  val l = x(arr_size)
  val f = x.slice(0,arr_size)
  LabeledPoint(l,Vectors.dense(f))
})

val unlabeledSet = data.filter{ case LabeledPoint(l,f) => l == -1 }

val labeledSet = data.filter{ case LabeledPoint(l,f) => l != -1 }

val sets = labeledSet.randomSplit(Array(0.8,0.2))
val trainSet = sets(0)
val testSet = sets(1)

trainSet.cache

---- MLlib Linear regression --------------

import org.apache.spark.mllib.regression.LinearRegressionWithSGD
val alg = new LinearRegressionWithSGD()
alg.setIntercept(true)
alg.optimizer.setNumIterations(200)

---- step size = 1.0
alg.optimizer.setStepSize(1.0)
val model = alg.run(trainSet)
model: intercept = NaN, numFeatures = 245

---- step size = 0.1
alg.optimizer.setStepSize(0.1)
val model = alg.run(trainSet)
model: intercept = NaN, numFeatures = 245

---- step size = 0.01
alg.optimizer.setStepSize(0.01)
val model = alg.run(trainSet)
intercept = -1.7004389326086608E241, numFeatures = 245

val validPredicts = testSet.map(x => (model.predict(x.features),x.label))

validPredicts.take(10)
res66: Array[(Double, Double)] = Array((-3.737906100038714E245,6.0), (-2.9326192449789727E245,2.35), (-3.4557765238160147E245,17.5), (-3.31750180355838E245,5.2), (-5.571314274554144E245,15.0), (-7.872708009732524E244,5.99), (-9.946574712455216E244,18.55), (-2.4679084145748533E245,4.25), (-7.320574604345365E245,4.0), (-3.3137264425568564E245,7.75))

---- step size = 0.001
alg.optimizer.setStepSize(0.001)
val model = alg.run(trainSet)
intercept = 2.18542957505878, numFeatures = 245

val validPredicts = testSet.map(x => (model.predict(x.features),x.label))

validPredicts.take(10)
res68: Array[(Double, Double)] = Array((907.5201473934146,6.0), (1047.918822267628,2.35), (-267.33877104837126,17.5), (262.8376611565601,5.2), (81.80616275177749,15.0), (-2908.636411986571,5.99), (62.32350348665,18.55), (844.5663704053504,4.25), (4823.885503327228,4.0), (526.5982798298658,7.75))

---- step size = 0.0001
alg.optimizer.setStepSize(0.0001)
val model = alg.run(trainSet)
intercept = 1.001690321812731, numFeatures = 245

val validPredicts = testSet.map(x => (model.predict(x.features),x.label))

validPredicts.take(10)
res70: Array[(Double, Double)] = Array((9.652616110419931,6.0), (6.929660673466294,2.35), (14.820300097394691,17.5), (10.377891130581162,5.2), (16.354250013005696,15.0), (2.1293277441028984,5.99), (2.14444247612218,18.55), (7.236556022245531,4.25), (5.617217412077971,4.0), (10.667892865399349,7.75))

import org.apache.spark.mllib.evaluation.RegressionMetrics
val validMetrics = new RegressionMetrics(validPredicts)
validMetrics.rootMeanSquaredError  //  10.749043117168199
validMetrics.meanSquaredError  // 115.54192793474104

---- step size = 0.0005
alg.optimizer.setStepSize(0.0005)
val model = alg.run(trainSet)
intercept = 1.0108544553269574, numFeatures = 245

val validPredicts = testSet.map(x => (model.predict(x.features),x.label))

validPredicts.take(10)
res74: Array[(Double, Double)] = Array((10.330364971746794,6.0), (7.990783418519296,2.35), (14.008708498906747,17.5), (10.861981867229447,5.2), (15.23026653462992,15.0), (4.6735076976934655,5.99), (4.240414874091378,18.55), (8.492135049779268,4.25), (5.835592932253089,4.0), (10.981336911555019,7.75))

import org.apache.spark.mllib.evaluation.RegressionMetrics
val validMetrics = new RegressionMetrics(validPredicts)
validMetrics.rootMeanSquaredError  //  10.658976589636037
validMetrics.meanSquaredError  // 113.61378193840909

---- step size = 0.00001
alg.optimizer.setStepSize(0.00001)
val model = alg.run(trainSet)
intercept = 1.000441691135075, numFeatures = 245

val validPredicts = testSet.map(x => (model.predict(x.features),x.label))

validPredicts.take(10)
res78: Array[(Double, Double)] = Array((5.910301167665991,6.0), (4.519191611855196,2.35), (7.719194447404183,17.5), (5.982313389318021,5.2), (9.292215145075126,15.0), (1.6968319906670093,5.99), (1.8164752519707048,18.55), (4.404332581858313,4.25), (6.1491385882194844,4.0), (6.092586608631229,7.75))

import org.apache.spark.mllib.evaluation.RegressionMetrics
val validMetrics = new RegressionMetrics(validPredicts)
validMetrics.rootMeanSquaredError  //  11.566222352391028
validMetrics.meanSquaredError  // 133.77749950494984


-------------------------- Decide to scale features because variabiliaty increases even reducing step size of model 

import org.apache.spark.mllib.feature.StandardScaler
val scaler = new StandardScaler(true, true).fit(trainSet.map(x => x.features))
 
val trainScaled = trainSet.map(x => LabeledPoint(x.label,scaler.transform(x.features)))
val validScaled = testSet.map(x => LabeledPoint(x.label, scaler.transform(x.features)))

---- MLlib Linear regression --------------

import org.apache.spark.mllib.regression.LinearRegressionWithSGD
val alg = new LinearRegressionWithSGD()
alg.setIntercept(true)
alg.optimizer.setNumIterations(200)

---- step size = 1.0
alg.optimizer.setStepSize(1.0)
val model = alg.run(trainScaled)
 intercept = 9.530683796104434, numFeatures = 245

val validPredicts = validScaled.map(x => (model.predict(x.features),x.label))

validPredicts.take(10)
res86: Array[(Double, Double)] = Array((5.191689530892167,6.0), (0.3683122231175613,2.35), (15.567595096473958,17.5), (4.9197340934055624,5.2), (13.714822002651818,15.0), (8.34526298513352,5.99), (18.999447731269075,18.55), (3.40718919295656,4.25), (7.265363247769578,4.0), (7.206405495322674,7.75))

import org.apache.spark.mllib.evaluation.RegressionMetrics
val validMetrics = new RegressionMetrics(validPredicts)
validMetrics.rootMeanSquaredError  //  5.475095941746165
validMetrics.meanSquaredError  // 29.976675571325323

---- step size = 0.1
alg.optimizer.setStepSize(0.1)
val model = alg.run(trainScaled)
intercept = 8.685332271439536, numFeatures = 245

val validPredicts = validScaled.map(x => (model.predict(x.features),x.label))

validPredicts.take(10)
res90: Array[(Double, Double)] = Array((5.81563180228455,6.0), (1.7693888507370108,2.35), (18.996088137276637,17.5), (3.9156906446153377,5.2), (11.285869930905644,15.0), (5.381597918054586,5.99), (17.492124851587693,18.55), (3.4217544275164835,4.25), (4.045878378573578,4.0), (7.108085834838471,7.75))

import org.apache.spark.mllib.evaluation.RegressionMetrics
val validMetrics = new RegressionMetrics(validPredicts)
validMetrics.rootMeanSquaredError  //  5.649420847048332
validMetrics.meanSquaredError  // 31.915955907064294

---- step size = 0.01
alg.optimizer.setStepSize(0.01)
val model = alg.run(trainScaled)
intercept = 3.0112846309233845, numFeatures = 245

val validPredicts = validScaled.map(x => (model.predict(x.features),x.label))

validPredicts.take(10)
res94: Array[(Double, Double)] = Array((1.7533173405149405,6.0), (-1.4110932747647738,2.35), (10.67840023573978,17.5), (1.165360965123804,5.2), (5.044793822222369,15.0), (0.18427999132801798,5.99), (9.052871633677093,18.55), (0.6515733100488412,4.25), (0.6647696137146015,4.0), (3.057357199258221,7.75))

import org.apache.spark.mllib.evaluation.RegressionMetrics
val validMetrics = new RegressionMetrics(validPredicts)
validMetrics.rootMeanSquaredError  //  9.832207293088922
validMetrics.meanSquaredError  // 96.67230025427098

--------------------

import org.apache.spark.mllib.linalg.Matrix
import org.apache.spark.mllib.linalg.distributed.RowMatrix

val vectors = rdd2.map( x => Vectors.dense(x))
val matrix = new RowMatrix(vectors)

val colsims = matrix.columnSimilarities()
val mat1 = colsims.toRowMatrix

import org.apache.spark.mllib.linalg.distributed.MatrixEntry
val transformedRDD = colsims.entries.map{case MatrixEntry(row: Long, col:Long, sim:Double) => ((row,col),sim)}

val rep = transformedRDD.sortBy(_._1).map(x => ((x._1._1,x._1._2),x._2))

var i = -1.0

rep.foreach( x => {
  val sim = x._2
  if (x._1._1 != i) { println
    print(f"$sim%.4f ")
    i = x._1._1
  } else print(f"$sim%.4f ")
})

0.5520 0.8327 0.5830 0.8772 0.4732
0.5121 0.3569 0.5443
0.3408
0.6872 0.9462 0.5573
0.7123 0.5742
0.6328
